{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VanshGupta-2404/ChatBOT_Hackathon/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Json File for translation\n"
      ],
      "metadata": {
        "id": "zv-U7jWaF6dd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezq_kMj9bFda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b3d20e-b786-4f12-8b05-e6b1b4da5d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show google-colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejjh80wSQQyI",
        "outputId": "2471f42b-42a2-4b24-c03a-808bdb60d829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Package(s) not found: google-colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrBCZPcP-8Ah"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import flask as Flask\n",
        "import json\n",
        "from flask import Flask,render_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVox4d7Z--wI"
      },
      "outputs": [],
      "source": [
        "file_path = 'drive/My Drive/content/Colab Notebooks/no_pii_grievance.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMkcvHcIWQC0"
      },
      "outputs": [],
      "source": [
        "json_converted= 'drive/My Drive/content/Colab Notebooks/translated_data.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIUv_pft6YJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d0c30f-103f-4d6e-d6b5-19c8aa22db0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries after preprocessing: 175782\n",
            "Sample preprocessed entry: Railways, ( Railway Board) >> Miscellaneous\r\n",
            "\r\n",
            "Railway Board/ Zone/ PSU/ PU/ Office : Railway Board - Railway Board\r\n",
            "-----------------------\r\n",
            "To\r\n",
            "The Railway Board\r\n",
            "SDAH  ER\r\n",
            "\r\n",
            "Location   Madhyamgram\r\n",
            "\r\n",
            "I  further to informing you that the temporary railway line crossing near Madhyamgram station  BT end.  is in a very bad condition. The stones on the side of the line have been moved far enough to cause great danger to the yrain,common people and train passengers at any time. Please look at the matter.  Although it was said that the place will be fixed but not done!\r\n",
            "Thanking you\r\n",
            "Yours truly\r\n",
            "Bhaskar  Mitra\r\n",
            "1.1.2023\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def load_json_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def preprocess_data(data):\n",
        "    preprocessed_data = []\n",
        "    for entry in data:\n",
        "        # Assuming each entry has a 'subject_context_text' field\n",
        "        subject_context_text = entry.get('subject_content_text', '').strip()\n",
        "        if subject_context_text:  # Checking if the subject_context_text field is not empty\n",
        "            # Other preprocessing steps such as lowercasing, removing special characters, etc. can be added here\n",
        "            preprocessed_data.append(subject_context_text)\n",
        "    return preprocessed_data\n",
        "\n",
        "# Example usage\n",
        "json_file_path = file_path  # Replace 'data.json' with the path to your JSON file\n",
        "data = load_json_file(json_file_path)\n",
        "preprocessed_data = preprocess_data(data)\n",
        "print(\"Number of entries after preprocessing:\", len(preprocessed_data))\n",
        "print(\"Sample preprocessed entry:\", preprocessed_data[0] if preprocessed_data else \"No data available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJ4Pjm60Ha96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fa4a9c-7865-45b0-976b-089f5964507c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=1855739f9bd2927a0f181b04c995c65db9521a45284e96b5c24e4fd188e91a8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "pip install langdetect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSxJ9zWOSwYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b317cd8a-d240-40c9-d1ee-44ded1a9e220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer as ps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0AzusjjzKaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66769c00-5c3c-467e-8004-69ea2e18cf49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2024.2.2)\n",
            "Installing collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install deep_translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOhIaw-nKuCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d07ab3-230b-470c-f996-adffcb77cced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.2.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans)\n",
            "  Downloading hstspreload-2024.2.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15715 sha256=579599eb83ac941d25fc83335cc0b9b24de15ec6b370c755bb28a680af517d73\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.2.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install googletrans"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSLATION USING GOOGLE APIS\n"
      ],
      "metadata": {
        "id": "8PMzLO8cG6qw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7YTGxW_0DGE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300):\n",
        "    translated_data = []\n",
        "    total_items = min(1000, len(data))  # Limit to the first 1000 items\n",
        "    for i in range(0, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        # Initialize a set to keep track of unique lines\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches\n",
        "translated_data = translate_subject_content_text(data)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n",
        "\n",
        "# Print the translated subject_content_text for the first 1000 items\n",
        "for item in translated_data[:1000]:\n",
        "    print(item.get('subject_content_text'))  # Use get method to avoid KeyErrors if 'subject_content_text' is missing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KkeeT_D7bME"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=1000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0r26A1EiB4m"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=2000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bpw5ZWrhxBfC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=3000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B64ITleLxHw1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=4000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c10FhMVo15LQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=5000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixflw7M318ZB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=6000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqUcYIuO1-IB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=7000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRPCvhbC3ss7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=8000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8jMvDic3wjZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=9000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqglNsU56vyW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=10000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO1_PPkW6xpw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=11000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GcnusTJ61MV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=12000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlLe880u_IRj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=13000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcA687YFAJXK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=14000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu7JgA5WAMiZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=15000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3wMdWvMBw0x"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=16000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxfjZO1jBy5o"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=17000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-dWAE5cGYS6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=18000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3lV5VSlGcbz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=19000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUlqU4YUGfFY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=20000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BgFbdnyufzo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=21000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdboicUbuiK5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=22000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaCZ2Uc3ukmV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=23000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bajqmEJNumdu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=24000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttrSUyJQupHe"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=25000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5VUKwIEurT3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=26000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1vMQ7G4uuWv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=27000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxEILRtWuv7O"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=28000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOsv2fhkuyx-"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=29000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2_FODsCu0ti"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=30000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiwvlXnzzFfP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=31000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZkjqoXizH2Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=32000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZFS_0vkzJv1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=33000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klk7QTdt695j"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=34000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on6JSEXj7FAI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=35000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv2Rm25g7HT5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=36000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DarN93n07Mjf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=37000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA1jpaix7PGC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=38000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTdcqdIE3KgE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=39000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GWKQNRq3Mpn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=40000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvmiIO9c3OrZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=41000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOF-gAP83Ql4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=42000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQk3t0Jw3TAz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=43000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaJr4E3X3U9a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=45000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpsgKuD63XUa"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=46000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ru0Ln6Z3ZfY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=47000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlRbyE483brc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=48000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3DkjkZD63e2i"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=49000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qTvdiVcd3hTZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=50000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VLDPdX1a6zpy"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=51000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "320a7t0b61lR"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=52000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TPOyajvY63W8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=53000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aucZKTSp65Sm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=54000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K3E11-IF67Hu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=55000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H3OoYUYR683o"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=56000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lfxVjZUB6-xk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=57000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K8hWU8ZE7ApA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=58000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ErptaRUN7Ce7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=59000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Kjd_Ocb67ESF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from deep_translator import GoogleTranslator\n",
        "import time\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "# Function to translate text to English with retry logic\n",
        "def translate_to_english(text, max_retries=5, max_words=300):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            # Limit the text to the first 300 words\n",
        "            text_to_translate = ' '.join(text.split()[:max_words])\n",
        "            translated_text = GoogleTranslator(source='auto', target='en').translate(text_to_translate)\n",
        "            return translated_text\n",
        "        except Exception as e:\n",
        "            print(f\"Error translating text: {e}\")\n",
        "            print(\"Retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(5)  # Increase the delay to 5 seconds before retrying\n",
        "    return text  # Return the original text if max retries reached\n",
        "\n",
        "# Function to translate the subject_content_text field in each item to English\n",
        "def translate_subject_content_text(data, batch_size=300, start_index=0):\n",
        "    translated_data = []\n",
        "    total_items = min(start_index + 1000, len(data))  # Limit to the next 1000 items\n",
        "    for i in range(start_index, total_items, batch_size):\n",
        "        batch = data[i:i+batch_size]\n",
        "        translated_batch = translate_batch(batch)\n",
        "        translated_data.extend(translated_batch)\n",
        "        print(f\"Translated {min(i+batch_size, total_items)} out of {total_items} items\")\n",
        "    return translated_data\n",
        "\n",
        "# Function to translate a batch of data\n",
        "def translate_batch(batch):\n",
        "    translated_batch = []\n",
        "    for item in batch:\n",
        "        translated_item = {}\n",
        "        if 'subject_content_text' in item and isinstance(item['subject_content_text'], str) and item['subject_content_text']:  # Check if subject_content_text exists, is a string, and is not None or empty\n",
        "            translated_item['subject_content_text'] = translate_to_english(item['subject_content_text'])\n",
        "        else:\n",
        "            continue  # Skip translation if subject_content_text is missing, empty, or None\n",
        "        translated_batch.append(translated_item)\n",
        "    return translated_batch\n",
        "\n",
        "# Function to save translated data to a JSON file\n",
        "def save_translated_data(translated_data, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        unique_lines = set()\n",
        "        for item in translated_data:\n",
        "            line = json.dumps(item)  # Convert item to JSON string\n",
        "            if line not in unique_lines:  # Check if the line is unique\n",
        "                file.write(line + '\\n')  # Write the line to the file\n",
        "                file.write('\\n')  # Add two line gaps\n",
        "                unique_lines.add(line)  # Add the line to the set of unique lines\n",
        "\n",
        "# Specify the path to your input JSON file and output JSON file\n",
        "input_file_path = file_path\n",
        "output_file_path = json_converted\n",
        "\n",
        "# Load JSON data\n",
        "data = load_json(input_file_path)\n",
        "\n",
        "# Translate the subject_content_text field in each item to English in batches, starting from index 1000\n",
        "translated_data = translate_subject_content_text(data, start_index=60000)\n",
        "\n",
        "# Save the translated data to a JSON file\n",
        "save_translated_data(translated_data, output_file_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO7w8kTyRg5b8uictlLqgFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}